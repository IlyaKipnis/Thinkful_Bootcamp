{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ElonTweetsSentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ydHvrgCuvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "# Using the following tutorial for sentiment analysis: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJJKcPzuMF4Y",
        "colab_type": "text"
      },
      "source": [
        "Since this is a sentiment analysis project, I googled up a tutorial on how to do that using NLTK's tweets corpus, and will follow that for the initial model. The goal, ultimately, is to classify the tweets of Elon Musk and use a markov generator to create Markov-generated Elon Musk tweets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7qkA7VqibNI",
        "colab_type": "code",
        "outputId": "fef68a11-117e-4aa4-8507-e73d8f634d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# get twitter samples from nltk\n",
        "\n",
        "nltk.download('twitter_samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3iJ3mp8idQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# group tweets into positive and negative\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTdBPcI6inJU",
        "colab_type": "code",
        "outputId": "62a7e3dd-9834-4645-93e5-1b730b9c2f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msUOgzg0i8fD",
        "colab_type": "code",
        "outputId": "99be366f-5874-4192-c72d-a4e4e2f4a9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIdsJHCBlPrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tokens from tweets\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTQSdB2blgSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use nltk for lemmatization\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "    return lemmatized_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43jvLP8HmZd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re, string\n",
        "\n",
        "# function to clean tweet text data--removes @s and links\n",
        "# also removes punctuation and stop words\n",
        "\n",
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Z2e6o9mflj",
        "colab_type": "code",
        "outputId": "cabca506-0583-4284-c018-2235cb4f35e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osn1fZxDm-ij",
        "colab_type": "code",
        "outputId": "96791166-0f21-4100-a935-e0fc880e97f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# get stop words\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCncgRZonewy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "#print(remove_noise(tweet_tokens[0], stop_words))\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfIyds1iniei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get most common positive words\n",
        "\n",
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUzE4zQCoIcN",
        "colab_type": "code",
        "outputId": "d2685554-5c12-4c31-f9c9-ff0ad66fafd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "freq_dist_pos = FreqDist(all_pos_words)\n",
        "print(freq_dist_pos.most_common(30))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253), ('u', 245), ('day', 242), ('like', 229), ('see', 195), ('happy', 192), (\"i'm\", 183), ('great', 175), ('hi', 173), ('go', 167), ('back', 163), ('know', 161), ('new', 147), ('make', 145), (':p', 139), ('<3', 135), ('one', 131), ('..', 129), ('time', 125), ('hope', 123), ('us', 115)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXRN3HFJ7Z95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gf7mJk-VNDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                     for tweet_dict in positive_tokens_for_model]\n",
        "\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsG3xsa2Vcff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DjH1CPvVqtF",
        "colab_type": "code",
        "outputId": "3672ae07-2786-4ed9-e83c-477fcd23c5eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "td = pd.DataFrame(train_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'yelaaaaaaa': True, ':(': True}</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'check': True, 'new': True, 'van': True, 'out...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'every': True, 'night': True, 'take': True, '...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'yes': True, 'always': True, 'selfish': True,...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{':(': True}</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0         1\n",
              "0                   {'yelaaaaaaa': True, ':(': True}  Negative\n",
              "1  {'check': True, 'new': True, 'van': True, 'out...  Positive\n",
              "2  {'every': True, 'night': True, 'take': True, '...  Negative\n",
              "3  {'yes': True, 'always': True, 'selfish': True,...  Positive\n",
              "4                                       {':(': True}  Negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPgePHzwWkSd",
        "colab_type": "code",
        "outputId": "4ebc6b41-7378-43f6-a5bb-b271e53480eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "# Use naive bayes classifier\n",
        "\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier\n",
        "classifier = NaiveBayesClassifier.train(train_data)\n",
        "\n",
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.996\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2056.5 : 1.0\n",
            "                      :) = True           Positi : Negati =   1662.7 : 1.0\n",
            "                follower = True           Positi : Negati =     36.8 : 1.0\n",
            "                     bam = True           Positi : Negati =     21.3 : 1.0\n",
            "                     sad = True           Negati : Positi =     19.6 : 1.0\n",
            "                  arrive = True           Positi : Negati =     18.8 : 1.0\n",
            "               community = True           Positi : Negati =     15.2 : 1.0\n",
            "                 welcome = True           Positi : Negati =     14.6 : 1.0\n",
            "                    blog = True           Positi : Negati =     13.8 : 1.0\n",
            "                    poor = True           Negati : Positi =     13.5 : 1.0\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_wnP9R6-3eI",
        "colab_type": "text"
      },
      "source": [
        "Let's test the model out by classifying Elon Musk's tweets. Elon Musk is a very interesting individual, so we'll start by classifying his tweets first (according to this NLTK model), and then use a markov chain generator to generate random tweets that look like Elon Musk may have written them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaRicHIjWswW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test on Elon Musk tweets\n",
        "\n",
        "elon_tweets = pd.read_csv(\"https://www.dropbox.com/s/s9tp2lv32l95r0g/user_tweets.csv?raw=1\", sep = \",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEcaZiMBaAJ-",
        "colab_type": "code",
        "outputId": "c48f1a59-3f51-438a-dcfe-a2c22cc490d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "elon_tweets.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>username</th>\n",
              "      <th>linktotweet</th>\n",
              "      <th>tweetembedcode</th>\n",
              "      <th>createdat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@highqualitysh1t I love the thought of a car d...</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>http://twitter.com/elonmusk/status/93704198630...</td>\n",
              "      <td>&lt;blockquote class=\"twitter-tweet\"&gt;&lt;p lang=\"en\"...</td>\n",
              "      <td>2017-12-02T19:33:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@novaspivack Asimov's Foundation books should ...</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>http://twitter.com/elonmusk/status/93709071522...</td>\n",
              "      <td>&lt;blockquote class=\"twitter-tweet\"&gt;&lt;p lang=\"en\"...</td>\n",
              "      <td>2017-12-02T22:46:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@novaspivack That's certainly the right way to...</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>http://twitter.com/elonmusk/status/93710961569...</td>\n",
              "      <td>&lt;blockquote class=\"twitter-tweet\"&gt;&lt;p lang=\"en\"...</td>\n",
              "      <td>2017-12-03T00:01:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To preserve the transcendent majesty &amp;amp; spe...</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>http://twitter.com/elonmusk/status/93739733099...</td>\n",
              "      <td>&lt;blockquote class=\"twitter-tweet\"&gt;&lt;p lang=\"en\"...</td>\n",
              "      <td>2017-12-03T19:05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@harrisonlingren @JW8888888 Busted</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>http://twitter.com/elonmusk/status/93739781363...</td>\n",
              "      <td>&lt;blockquote class=\"twitter-tweet\"&gt;&lt;p lang=\"en\"...</td>\n",
              "      <td>2017-12-03T19:07:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...            createdat\n",
              "0  @highqualitysh1t I love the thought of a car d...  ...  2017-12-02T19:33:00\n",
              "1  @novaspivack Asimov's Foundation books should ...  ...  2017-12-02T22:46:00\n",
              "2  @novaspivack That's certainly the right way to...  ...  2017-12-03T00:01:00\n",
              "3  To preserve the transcendent majesty &amp; spe...  ...  2017-12-03T19:05:00\n",
              "4                 @harrisonlingren @JW8888888 Busted  ...  2017-12-03T19:07:00\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmP4XREtzbDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BRVEkW9zzK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classify Elon's tweets\n",
        "\n",
        "classifications = []\n",
        "elon_tweets = elon_tweets.dropna()\n",
        "for tweet in elon_tweets['text']:\n",
        "\n",
        "  custom_tokens = remove_noise(word_tokenize(tweet))\n",
        "  classifications.append(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOv_EK3A0nm4",
        "colab_type": "code",
        "outputId": "b4a0ba5d-9165-4428-b38a-15f740981eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        }
      },
      "source": [
        "elon_tweets['class'] = classifications\n",
        "elon_tweets.head()\n",
        "neg_elon_tweets = elon_tweets[elon_tweets['class'] == 'Negative']\n",
        "pos_elon_tweets = elon_tweets[elon_tweets['class'] == 'Positive']\n",
        "print(neg_elon_tweets.shape)\n",
        "print(pos_elon_tweets.shape)\n",
        "elon_tweets.head()\n",
        "del elon_tweets['linktotweet']\n",
        "del elon_tweets['tweetembedcode']\n",
        "del elon_tweets['username']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3174, 3)\n",
            "(3722, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'linktotweet'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-6f28571c5332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_elon_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0melon_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0melon_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'linktotweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0melon_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweetembedcode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0melon_tweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'username'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3757\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3758\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3761\u001b[0m         \u001b[0;31m# delete from the caches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \"\"\"\n\u001b[0;32m-> 1002\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'linktotweet'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vGWuoaa7b5Y",
        "colab_type": "code",
        "outputId": "4204a0f0-9318-4e64-ae78-995f15645d27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "elon_tweets.head(30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>createdat</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@highqualitysh1t I love the thought of a car d...</td>\n",
              "      <td>2017-12-02T19:33:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@novaspivack Asimov's Foundation books should ...</td>\n",
              "      <td>2017-12-02T22:46:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@novaspivack That's certainly the right way to...</td>\n",
              "      <td>2017-12-03T00:01:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To preserve the transcendent majesty &amp;amp; spe...</td>\n",
              "      <td>2017-12-03T19:05:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@harrisonlingren @JW8888888 Busted</td>\n",
              "      <td>2017-12-03T19:07:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@IvanEscobosa Yes</td>\n",
              "      <td>2017-12-03T19:07:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Hat</td>\n",
              "      <td>2017-12-03T19:20:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Every 5000th buyer of our boringly boring hat ...</td>\n",
              "      <td>2017-12-03T19:24:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>@TheRealUtkarsh Because it's stupid</td>\n",
              "      <td>2017-12-03T19:29:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>@maralkalajian Maybe</td>\n",
              "      <td>2017-12-03T19:28:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>@NefuDaBoss Beyond anything you can imagine</td>\n",
              "      <td>2017-12-03T19:37:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>@TheRealUtkarsh And, at the risk of stating th...</td>\n",
              "      <td>2017-12-03T19:33:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>That special hat delivery will take place deep...</td>\n",
              "      <td>2017-12-03T20:01:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>@sm_carl Deal</td>\n",
              "      <td>2017-12-03T22:21:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>The *real* money comes from merchandising. I l...</td>\n",
              "      <td>2017-12-03T22:24:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>@FortuneTech Do it</td>\n",
              "      <td>2017-12-07T17:05:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>@tiamaria68uk Yes</td>\n",
              "      <td>2017-12-08T01:43:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>On Tuesday, SpaceX will attempt to fly both an...</td>\n",
              "      <td>2017-12-08T03:23:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>On Tuesday, SpaceX will attempt to refly both ...</td>\n",
              "      <td>2017-12-08T03:24:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>@tiamaria68uk Plus a towel and a sign saying \"...</td>\n",
              "      <td>2017-12-08T05:37:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Over 30,000 hats sold!</td>\n",
              "      <td>2017-12-11T03:07:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>After 50k hats, we will start selling The Bori...</td>\n",
              "      <td>2017-12-11T03:11:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>I know it's a little off-brand, but kids love it</td>\n",
              "      <td>2017-12-11T07:47:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>@Justin_Levy04 Safest flamethrower ever</td>\n",
              "      <td>2017-12-11T07:49:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>And those are just the really big ones. The Pe...</td>\n",
              "      <td>2017-12-11T23:13:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>@DMC_Ryan I agree</td>\n",
              "      <td>2017-12-11T23:20:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>35,000</td>\n",
              "      <td>2017-12-12T16:34:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>@kaaningilamo Our existence cannot just be abo...</td>\n",
              "      <td>2017-12-13T20:32:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>It is high time that humanity went beyond Eart...</td>\n",
              "      <td>2017-12-13T20:29:00</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10 hat aficionados, in appreciation for their ...</td>\n",
              "      <td>2017-12-14T19:24:00</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 text  ...     class\n",
              "0   @highqualitysh1t I love the thought of a car d...  ...  Positive\n",
              "1   @novaspivack Asimov's Foundation books should ...  ...  Positive\n",
              "2   @novaspivack That's certainly the right way to...  ...  Negative\n",
              "3   To preserve the transcendent majesty &amp; spe...  ...  Negative\n",
              "4                  @harrisonlingren @JW8888888 Busted  ...  Negative\n",
              "5                                   @IvanEscobosa Yes  ...  Positive\n",
              "6                                                 Hat  ...  Negative\n",
              "7   Every 5000th buyer of our boringly boring hat ...  ...  Positive\n",
              "8                 @TheRealUtkarsh Because it's stupid  ...  Negative\n",
              "9                                @maralkalajian Maybe  ...  Negative\n",
              "10        @NefuDaBoss Beyond anything you can imagine  ...  Positive\n",
              "11  @TheRealUtkarsh And, at the risk of stating th...  ...  Negative\n",
              "12  That special hat delivery will take place deep...  ...  Negative\n",
              "13                                      @sm_carl Deal  ...  Negative\n",
              "14  The *real* money comes from merchandising. I l...  ...  Negative\n",
              "15                                 @FortuneTech Do it  ...  Negative\n",
              "16                                  @tiamaria68uk Yes  ...  Positive\n",
              "17  On Tuesday, SpaceX will attempt to fly both an...  ...  Positive\n",
              "18  On Tuesday, SpaceX will attempt to refly both ...  ...  Positive\n",
              "19  @tiamaria68uk Plus a towel and a sign saying \"...  ...  Negative\n",
              "20                             Over 30,000 hats sold!  ...  Negative\n",
              "21  After 50k hats, we will start selling The Bori...  ...  Negative\n",
              "22   I know it's a little off-brand, but kids love it  ...  Positive\n",
              "23            @Justin_Levy04 Safest flamethrower ever  ...  Negative\n",
              "24  And those are just the really big ones. The Pe...  ...  Negative\n",
              "25                                  @DMC_Ryan I agree  ...  Positive\n",
              "26                                             35,000  ...  Negative\n",
              "27  @kaaningilamo Our existence cannot just be abo...  ...  Positive\n",
              "28  It is high time that humanity went beyond Eart...  ...  Positive\n",
              "29  10 hat aficionados, in appreciation for their ...  ...  Negative\n",
              "\n",
              "[30 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n08VrV261-7a",
        "colab_type": "text"
      },
      "source": [
        "Elon apparently has nearly as many negative tweets as positive tweets according to the NLTK tweet corpus. This is probably not that good of a model. It seems the NLTK twitter corpus was just not that good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD1fP9Kq82Uu",
        "colab_type": "code",
        "outputId": "5337a455-2a28-4f74-f9e4-01d7466331aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "import spacy\n",
        "!pip install markovify\n",
        "import markovify"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting markovify\n",
            "  Downloading https://files.pythonhosted.org/packages/de/c3/2e017f687e47e88eb9d8adf970527e2299fb566eba62112c2851ebb7ab93/markovify-0.8.0.tar.gz\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: markovify\n",
            "  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markovify: filename=markovify-0.8.0-cp36-none-any.whl size=10694 sha256=ca8a650180069eeb9ec4dd5df4ffa3f56752c6aaee8882909120a65e399f9bef\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/a8/92/35e2df870ff15a65657679dca105d190ec3c854a9f75435e40\n",
            "Successfully built markovify\n",
            "Installing collected packages: unidecode, markovify\n",
            "Successfully installed markovify-0.8.0 unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTxYl2Nr9AIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "# below is necessary to avoid memory error of SpaCy\n",
        "nlp.max_length = 20000000\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "twitter_doc = nlp(\" \".join(elon_tweets.text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-9tJV8-EXSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elon_negative_doc = nlp(\" \".join(elon_tweets[elon_tweets[\"class\"]==\"Negative\"].text))\n",
        "elon_negative_sents = \" \".join([sent.text for sent in elon_negative_doc.sents if len(sent.text) > 1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCvUdz6tE04N",
        "colab_type": "code",
        "outputId": "b0e6f894-b32a-4e34-8379-44e670bf4047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "elon_negative_generator = markovify.Text(elon_negative_sents, state_size = 3)\n",
        "\n",
        "# three randomly generated negative sentences\n",
        "for i in range(20):\n",
        "    print(elon_negative_generator.make_sentence(tries=100))\n",
        "\n",
        "# three randomly-generated negative sentences of no more than 100 characters\n",
        "for i in range(20):\n",
        "    print(elon_negative_generator.make_short_sentence(200, tries=100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@kimbal you too @austinbarnard45 @flcnhvy @Joe__Wakefield @tjq1190 @tyger_cyber @fawfulfan @_Mikemo This is a significant policy difference in Italy vs most other countries.\n",
            "@BrandonJHavard No @arctechinc @BrandonJHavard No @CarolineGee8 @NathanBomey @USATODAY @Tesla @mayemusk That was a mistake.\n",
            "@PPathole Black &amp; white interior available only for Model 3 Performance https://t.co/Vejb9fTY5Q RT @SpaceX: Dragon is holding at the capture point ~10 meters from the @Space_Station.\n",
            "We can solar power all of human civilization with a tiny % of the US prob helps convert some naysayers.\n",
            "This is simple replacement of the Autopilot team has been working all weekend to resolve last minute issues.\n",
            "@DMC_Ryan Can someone please do that!? I would def recommend this @williamwinters @austinhopperrrr @maysacha @thanr @JamesWorldSpace Exactly.\n",
            "RT @CRcars: Consumer Reports tested the @Tesla Model 3 and 5-star safety ratings: name a more iconic duo.\n",
            "@martinengwicht @Erdayastronaut @DiscoverMag Even connectivity at my house with the Tesla AI/autopilot team in about four weeks.\n",
            "Next stop, the restaurant at the end of the year by The Detroit News! https://t.co/inyVuf1CzL RT @Tesla: In 8 years we've gone from building 500 vehicles per year in USA alone https://t.co/xoaK6v6T6f @ValueAnalyst1 @peterdevietien @scottwww @karpathy @Tesla @nvidia Or maybe pegging @flcnhvy Is that a real billboard?\n",
            "@J_ump_er @neiltyson @Space_Station To be precise, my mistake.\n",
            "After struggling to make the awesome version of Model 3 Performance is next-level.\n",
            "@PPathole @Teslarati Yes, rather embarrassing tbh RT @SpaceX: Dragon returned home yesterday after its second month-long stay at the @Space_Station.\n",
            "Will be done free of charge for those who haven't purchased it is starting to roll out.\n",
            "None\n",
            "RT @NASA: More than 5,600 pounds of @ISS_Research and supplies.\n",
            "All major highways in Texas will have Superchargers, all the way into your garage https://t.co/DPnkC80NWH First Boring Brick store opening in ~2 months.\n",
            "@wonderofscience @Treebeard1671 @Erdayastronaut @keego73 Yes.\n",
            "@thehumanwire @InsideEVs @28delayslater Probably a month or two, although it may be a little racing game in the racing game in the car you need to stop every three hours for bathroom and food anyway.\n",
            "ET. https://t.co/0qHhHzD4Js Atmospheric entry at 17,000 mph is like a beaver with small ideas, but it has a certain quiet dignity ...\n",
            "@TeslaCharts Then you'll be fine @TeslaCharts I don't want to write Tesla a check.\n",
            "Tesla will be there as soon as we have the data.\n",
            "Thousands of US auto startups, but only Ford &amp; Tesla are the only 2 American car companies to avoid bankruptcy.\n",
            "Thanks @elonmusk and everyone at Tesla for making the finest car I've ever owned or driven.\n",
            "Will feel like folding space from one part of a city bus, it will be something like that.\n",
            "So many bots!! True https://t.co/CS6semhKm6 @JaneidyEve @slashdot Yes, if there is someone with a pattern of doing so.\n",
            "Our main issue here in Boca is that it will be ~150t to LEO fully reusable @NYCHealthSystem @Tesla Happy we could be helpful!\n",
            "@jeffborden_cga @marc_benton @MrTommyCampbell @Tesla When prices go down, those who already bought don't want to write Tesla a check.\n",
            "@flcnhvy No, he's still much faster @teslaownersSV @Benioff Boring Co is launching a whole product line of DIY watchtowers.\n",
            "@WorldAndScience Common myth, but incorrect @John_Gardi @Erdayastronaut @NotEricRalph @MartianDays @torybruno @AerojetRdyne @elon Both at the same time!\n",
            "No more need to import fossil fuels for electricity, which is a super tiny % of Earth's surface, which is a strategic vulnerability.\n",
            "@teslaownersSV @elontimes Excessive shades of grey @Erdayastronaut @13ericralph31 @HarryStoltz1 @flcnhvy Since Raptor produces 200 tons of force, cost per ton over time would actually be ~$1.\n",
            "@SpaceForceDoD Starfleet begins @engineers_feed I try so hard to be way better.\n",
            "This vehicle has seen a lot of people whose judgment I respect have suggested this.\n",
            "@wonderofscience @Treebeard1671 @Erdayastronaut @keego73 Yes.\n",
            "For now, it's slightly disadvantageous to have Tesla FSD computer or Tesla Nvidia-based computer.\n",
            "The awareness is just a matter of courtesy &amp; fairness.\n",
            "EDT, or 3:30 UTC on June 25 - https://t.co/gtC39uBC7z https://t.co/NTblKjmFxT Falcon Heavy on Feb 6 from Apollo launchpad 39A at Cape Kennedy.\n",
            "@RossSheingold @Tesla Upgrading hardware is not important for a few days Order Tesla Solar + Powerwall battery for 24/7 clean power with no blackouts.\n",
            "@DMC_Ryan Yes, Model 3 Performance is next-level.\n",
            "Our main issue here in Boca is that it will be information &amp; entertainment, not trickery @Erik2be @Teslarati Sorry to hear that.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cllgk9GcHzTf",
        "colab_type": "text"
      },
      "source": [
        "The sentiments in the Markov-generated Elon negative tweets do not seem to be negative *at all*. It seems that the NLTK twitter corpus is extremely lacking once the luxury of emoticons gets lost. It seems Elon Musk is generally a pretty positive guy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfWthtm1JQon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "elon_positive_doc = nlp(\" \".join(elon_tweets[elon_tweets[\"class\"]==\"Positive\"].text))\n",
        "elon_positive_sents = \" \".join([sent.text for sent in elon_positive_doc.sents if len(sent.text) > 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE996aWqJ3cB",
        "colab_type": "code",
        "outputId": "57d85dd1-14a5-4ae1-d096-5c49985370a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "elon_positive_generator = markovify.Text(elon_positive_sents, state_size = 3)\n",
        "\n",
        "# three randomly generated negative sentences\n",
        "for i in range(20):\n",
        "    print(elon_positive_generator.make_sentence(tries=100))\n",
        "\n",
        "# three randomly-generated negative sentences of no more than 100 characters\n",
        "for i in range(20):\n",
        "    print(elon_positive_generator.make_short_sentence(200, tries=100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@Kristennetten @mayapolarbear Technically, his bro RT @wonderofscience : This is what it would look a bit like a Mars simulator.\n",
            "Really need to bring it to a Tesla service center. https://t.co/KCIFtliZr8 @ElectrekCo I was just a simple nucleotide, drifting alone in small crevice with 3 trillion siblings.\n",
            "Essence of a good editor @lexiheft @Tesla Coming soon @CathieDWood Thank you for your trust in the @SpaceX team.\n",
            "@Erdayastronaut @SpaceX Super proud of Tesla Autopilot team!\n",
            "@NASA @SpaceX @Space_Station @Commercial_Crew Most likely, but this is an important clarification @ThePhoenixFlare @MKBHD @HyperChangeTV Yeah, news is actually super good.\n",
            "It's amazing. https://t.co/eLqr4pLeIX @thanr Sure @aparanjape Prob early next year Just finished an engineering review with SpaceX Propulsion.\n",
            "Weather is over 90% favorable for today's launch attempt - https://t.co/gtC39uBC7z RT @NASA: How many worlds exist outside our solar system?\n",
            "Aiming to finish initial construction this summer, start Model 3 production ramp &amp; trying to get to profitability.\n",
            "RT @SpaceX: Webcast of Falcon 9 launch of Dragon targeted for 12:29 p.m.\n",
            "Engineering team also getting feedback from the British dive team on how to improve the link quality so it can be deadly to pro divers when high.\n",
            "Adding fold out solar wings would generate 30 to 40 rocket cores for ~300 missions over 5 years.\n",
            "We are building the Starship prototypes locally at our launch site in Texas https://t.co/MtxkuhDDdE @grafikhure_de @StevenHardison Skin will get too hot for paint.\n",
            "Apply at https://t.co/84BkZvuBn8 Looking forward to using the new Iridium constellation.\n",
            "Aero surfaces &amp; high gimbal angle main engines for landing orientation, so won't need high thrust reaction control.\n",
            "EDT, or 4:29 UTC. https://t.co/gtC39uBC7z https://t.co/GziYEyZAxN RT @SpaceX: Successful deployment of 60 Starlink satellites from Pad 40 in Florida.\n",
            "Obsidian Black &amp; Metallic Silver will still be available as an upgrade from service.\n",
            "Reminder to US buyers that the $7500 tax credit drops in half for Tesla on July 1.\n",
            "Adding fold out solar wings would generate 30 to 40 rocket cores for ~300 missions over 5 years.\n",
            "Two seater electric ATV designed to work with them @JaneidyEve Mini-sub arriving in about 17 hours.\n",
            "@reto_siegrist @flcnhvy @thirdrowtesla Yes @justpaulinelol @reto_siegrist @flcnhvy @thirdrowtesla Yes @justpaulinelol @reto_siegrist @flcnhvy @thirdrowtesla Definitely!\n",
            "RT @wonderofscience: Red sunset on a red planet.\n",
            "As with vehicle safety, it will be in a future situation.\n",
            "@ChrisEvans Great thread RT @tsrandall: In the second half of 2018, the Model 3 is the best-selling electric car, despite no advertising or paid endorsements.\n",
            "@vincent13031925 @TeslaBull @scottwww @stetopinini @S_Padival @lopezlinette Indeed, very simple question.\n",
            "2019 free cash flow Super proud of SpaceX propulsion/test/materials team!\n",
            "Weather is 60% favorable for tonight's four-hour launch window, which opens Tuesday, August 7 at 1:18 a.m.\n",
            "2019 free cash flow Super proud of SpaceX propulsion/test/materials team!\n",
            "#Tesla #Model3 # RWD https://t.co/G3FnR3pwRi RT @Tesla: Model 3 recently earned the 2020 IIHS Top Safety Pick+ Award.\n",
            "If not Tesla, please take a look at the timeline: https://t.co/cP1HuVwg6c https://t.co/fJCvzTo8Y7 RT @AstroAnnimal : Yes buddy, that's your Mother Earth.\n",
            "RT @SpaceX: Falcon 9 and Crew Dragon are vertical ahead of today's attempt - https://t.co/gtC39uBC7z RT @NASA: How many worlds exist outside our solar system?\n",
            "RT @Hyperloop: Student teams are arriving at @SpaceX for the 2018 Hyperloop Pod Competition Technically, alcohol is a solution Just saw that on a Tesla.\n",
            "There need to be right.\n",
            "RT @SpaceX: Falcon 9 and RADARSAT Constellation Mission have rolled out to the Tesla team, who have been working crazy hard.\n",
            "Great word-of-mouth is why Model 3 is designed to burn @nichegamer Nice @justpaulinelol @Tesla Same, except where limited by EU regulations.\n",
            "Also, thank you to all Tesla supporters around the world have different rules &amp; processes, so just takes time.\n",
            "EST, 1:45 UTC - https://t.co/gtC39uBC7z https://t.co/UbpdQrJeNx RT @Tesla: Model 3 configurator is now open to left-hand drive countries in Europe.\n",
            "A lot of the misunderstandings @WillFealey No problem if you want a display car, order at https://t.co/46TXqRJ3C1 or visit our stores.\n",
            "If all goes well, each launch of 60 satellites will generate more power than we needed, 2.7kW sending back to the grid!\n",
            "Both computers will be used for GF4.\n",
            "Hopefully, ready to test in a few months @brandonbernicky @tsrandall Not good enough yet.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKOir9Z3LoxG",
        "colab_type": "text"
      },
      "source": [
        "It seems the positive *and* negative tweets have about similar sentiments. There seems to be lots of talk about Elon's companies."
      ]
    }
  ]
}